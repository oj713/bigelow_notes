---
title: "Improving Models - tmwr.org"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidymodels)
library(modeldata)

data(ames)

ames <- ames |>
  mutate(Sale_Price = log10(Sale_Price))

set.seed(137)
ames_split <- initial_split(ames, prop = 3/4, strata = Sale_Price)
training <- training(ames_split)
testing <- testing(ames_split)
set.seed(NULL)

rec <- 
  recipe(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + 
           Bldg_Type + Latitude + Longitude,
         data = training) |>
  step_log(Gr_Liv_Area, base = 10) |>
  step_other(Neighborhood, threshold= 0.01, id = "neightidy") |>
  step_dummy(all_nominal_predictors()) |>
  step_interact( ~ Gr_Liv_Area:starts_with("Bldg_Type_")) |>
  step_ns(Latitude, Longitude, deg_free = 20)

model <- linear_reg() |> 
  set_engine("lm") 

lm_wkf <- workflow() |>
  add_model(model) |>
  add_recipe(rec) 

lm_fit <- fit(lm_wkf, training)
```

Using the `rec`, `model`, `lm_wkf`, and `lm_fit` defined in tmwr_basics.R

## Resampling for Evaluating Performance ##

* usually, we need to understand the effectiveness of the model *before* the test set. 
  * **Idea**: use the training set
  
**Low Bias Models**

* predictive models that can learn complex trends from data
* high predictive capacity can sometimes mean that the model basicallly memorizes the training set. 
  * If we tried to evaluate the model using the training set, the results might be much higher than actual effectiveness. 
  
**Re-predicting using the training set is generally a bad idea.**

### Example ###

```{r}
# creating a random forest workflow 
rf <- workflow() |>
  add_formula(Sale_Price ~ Neighborhood + Gr_Liv_Area + Year_Built + Bldg_Type + Latitude + Longitude) |>
  add_model(rand_forest(trees = 1000, 
                        engine = "ranger", 
                        mode = "regression")) |>
  fit(data = training)

# Produces "apparent error rate" 
estimate_error <- function(model, data) {
  cl <- match.call()
  reg_metrics <- metric_set(rmse, rsq) 
  
  model |>
    predict(data) |>
    bind_cols(data |> select(Sale_Price)) |>
    reg_metrics(Sale_Price, .pred) |>
    select(-.estimator) |>
    mutate(object = as.character(cl$model),
           data = as.character(cl$data))
}

# estimating error rate for random forest using training data 
estimate_error(rf, training)
estimate_error(lm_fit, training)
```

Based on these results, `rf` is better at predicting sale prices - RMSE is 2x better. However, when we apply random forest to the test set:

```{r}
estimate_error(rf, testing)
estimate_error(lm_fit, testing)
```

The new estimate RMSE is much worse for random forest (which is complex), and about the same for linear regression (which is simple). 

## Resampling Methods ##

**V-fold cross validation**
```{r}
set.seed(103)
ames_folds <- vfold_cv(training, v = 5, repeats = 2)
set.seed(NULL)
ames_folds
```

**Generating fits**
```{r}
control <- control_resamples(save_pred = TRUE, save_workflow = TRUE) 

set.seed(87)
rf_res <- rf |>
  fit_resamples(resamples = ames_folds, control = control)

rf_res

# averages the resampling - for results per fold, use summarize = FALSE
collect_metrics(rf_res)

assess_res <- collect_predictions(rf_res)

assess_res

assess_res |>
  ggplot(aes(x = Sale_Price, y = .pred)) +
  geom_point(alpha = .15) +
  geom_abline(color = "orange") +
  coord_obs_pred() +
  ylab("Predicted")
```

By looking at the plot, there are two houses that are sig. overpredicted by model ~ can determine which houses using `assess_res`

```{r}
over_predicted <- assess_res |>
  mutate(residual = Sale_Price - .pred) |>
  arrange(desc(abs(residual))) |>
  slice(1:2)

training |>
  slice(over_predicted$.row) |>
  select(Gr_Liv_Area, Neighborhood, Year_Built, Bedroom_AbvGr, Full_Bath)
```






